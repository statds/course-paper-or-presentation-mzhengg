\documentclass[12pt, letterpaper, titlepage]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{colorlinks = true, linkcolor = blue, citecolor=blue, urlcolor = blue}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{lipsum}

\usepackage[pagewise]{lineno}
%\linenumbers*[1]
% %% patches to make lineno work better with amsmath
\newcommand*\patchAmsMathEnvironmentForLineno[1]{%https://www.overleaf.com/project/634313f2f2e4439ea3a087af
 \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
 \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
 \renewenvironment{#1}%
 {\linenomath\csname old#1\endcsname}%
 {\csname oldend#1\endcsname\endlinenomath}}%
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
 \patchAmsMathEnvironmentForLineno{#1}%
 \patchAmsMathEnvironmentForLineno{#1*}}%

\AtBeginDocument{%
 \patchBothAmsMathEnvironmentsForLineno{equation}%
 \patchBothAmsMathEnvironmentsForLineno{align}%
 \patchBothAmsMathEnvironmentsForLineno{flalign}%
 \patchBothAmsMathEnvironmentsForLineno{alignat}%
 \patchBothAmsMathEnvironmentsForLineno{gather}%
 \patchBothAmsMathEnvironmentsForLineno{multline}%
}

% control floats
\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}

\newcommand{\jy}[1]{\textcolor{blue}{JY: #1}}
\newcommand{\eds}[1]{\textcolor{red}{EDS: (#1)}}


\title{Fake Amazon reviews detection using natural language processing and supervised learning}

\author{Michael Zheng\\
  Jun Yan\\[1ex]
  Department of Statistics, University of Connecticut\\
}
\date{}

\begin{document}
\maketitle
\doublespace

\begin{abstract}
    Customer reviews play an important role of influencing one's purchase decision. Consumers trust the reputation of the brand or product based on reviews, while sellers are interested in making sales. Positive reviews can increase sales, and vice versa. Thus, sellers can, and often do, abuse the review system for their own gain. This paper seeks to develop a support vector machine model to accurately detect fake Amazon reviews using natural language processing techniques.
    
\bigskip
\noindent{\sc Keywords}:
natural language processing;
sentiment analysis;
supervised learning;
support vector machine.

\end{abstract}

\section{Introduction}
\label{sec:intro}

Reviews are an integral part of an Amazon customer's shopping experience. Products with good ratings and feedback are deemed to be more trustworthy and help increase sales. But, this review system is difficult to moderate and can be abused by sellers. With so much to gain and little to lose for vendors, it's important to detect fake reviews from genuine ones in order to make the Amazon marketplace fair.
    
Sentiment analysis is a method of quantifying positive, neutral, or negative opinions to a piece of text. Reviews can be assigned an opinion type based on the presence of certain words. Furthermore, reviews that are written to promote a product are positive in nature, while those written to demote a product, usually by a competitor, are negative, as demonstrated by \citep{pendyala_2019}.

Previous studies, such as this one carried out by \citep{article}, demonstrated success in detecting fake reviews from genuine ones using sentiment analysis and supervised learning to classify reviews. However, reviews in their raw form cannot be interpreted by classification models. They must be processed by removing stopwords, commonly used English words such as "a" and "the", and then tokenized by splitting sentences into smaller fragments that can be more easily assigned meaning. 

The main contribution of this paper is to corroborate the findings of prior research papers on this topic. That is, NLTK, an open source natural language processing framework, will be used to process Amazon reviews by removing stopwords and tokenizing sentences because prior studies have shown this is an effective way to detect fake reviews. Then, a support vector machine (SVM), a popular supervised learning technique for classification problems, will be trained using a Amazon reviews dataset from Kaggle to detect fake reviews in a test set.

The paper will be organized as followed: Section 2 provides a description of the dataset used. Section 3 presents the methodology. Section 4 discusses the results of the study. Section 5 presents the conclusion and future works.

\section{Data Collection}
\label{sec:data}

The data that will be used for this paper contains 21,000 Amazon reviews from 2018 across a number of product categories. The reviews are labelled as either a fake or real review. Each review has a corresponding rating and review title as well. There is also product metadata consisting of the product category and product title. This dataset was sourced from \href{https://www.kaggle.com/datasets/lievgarcia/amazon-reviews}{Kaggle}. 

The dataset contains features and corresponding labels for training and testing a support vector machine (SVM) to detect fake Amazon reviews. In total, there are 9 columns, 7 are categorical and 2 are numerical. The data was explored and visualized using the pandas library in Python. Specifically, each variable was grouped by the label, which is a 0 if the review is real and a 1 if the review is fake, to assess the frequency of the values of each variable in the fake and real reviews.

\section{Methods}
\label{sec:methods}
The data was processed to remove irrelevant or redundant information. A simple approach to sentiment classification was attempted. Any rating that is greater than 3 stars was assigned a value of 1 and was attributed to a positive opinion. Concurrently, ratings that are exactly 3 stars were negated as they were neutral opinions and the remaining ratings are assigned a 0 and considered a negative opinion. The number of usable data points now stands at 19,132.

An imbalance between positively and negatively opinionated ratings was present (16183:2949). 20\% of the ratings with a newly assigned value of 1 were selected, which was approximately 3237 data points, to better balance them with the number of 0 ratings, 2949. The imbalance was addressed to ensure that the SVM model does not ignore the minority class and overfit to the majority class. This resulting subset of the dataset contains 6186 data points and was used for further processing

The review text was processed by removing stop words, which are commonly used English words. For instance, the root word of "running" is "run", which the NLTK framework can recognize and remove the "ing". This is done to ensure that the SVM model only focuses on the important information of a word. This is done for each word in a review for every review in the dataset in a process called lemmatization. With each review converted to a list of words in its base form, bigrams, or pairs of English words that commonly go together, were generated for each possible pair of words in the list. These bigrams convey more information about the general sentiment of the reviews through the frequency of such pairs occurring in a body of text.

Features that are irrelevant to detecting fake reviews were removed. The features that were kept included the rating, verified purchase status, product category, and review. These features were stored in a feature vector for each observation alongside its corresponding label. The data was then randomly split into a test and training set based on a proportion p, which is the proportion of data from the 6186 data points to devote to the training set. For the results generated in this paper, 80\% of the data was assigned to the training set. This proportion was arbitrarily chosen to be greater than the testing set to ensure that the SVM model finds patterns and learns from the data.

A SVM model was fitted to the training data and then validated on the test set using the scikit-learn library in Python.

\section{Results}
\label{sec:results}

In this section, the results from the SVM model to classify fake and real Amazon reviews from the Kaggle dataset are presented. A confusion matrix is used to assess the performance. 

\begin{tabular}{l|l|c|c|c}
\multicolumn{2}{c}{}&\multicolumn{2}{c}{Predicted Reviews}&\\
\cline{3-4}
\multicolumn{2}{c|}{}&Fake&Real&\multicolumn{1}{c}{Total}\\
\cline{2-4}
\multirow{Actual Reviews}& Fake & $717$ & $211$ & $928$\\
\cline{2-4}
& Real & $56$ & $254$ & $310$\\
\cline{2-4}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{Total} & \multicolumn{1}{c}{$773$} & \multicolumn{    1}{c}{$465$} & \multicolumn{1}{c}{$1238$}\\
\end{tabular}

\vspace{8mm}

Accuracy, recall, and precision are common measures to evaluate the performance of a classification model. They can be calculated using the information in the confusion matrix. That is, TP = predicted (fake) + actual (fake), TN = predicted (real) + actual (real), FP = predicted (fake) + actual (real), FN = predicted (real) + actual (fake), P = total of actual fake reviews, N = total of actual real reviews.

\[Accuracy = \dfrac{TP + TN}{P + N} = 0.784\]

Accuracy is defined as the degree to which the model classifies all the reviews properly. In this case, the model classified 78.4$\%$ of the reviews in the test set properly. 

\[Precision = \dfrac{TP}{TP + FP} = 0.927\]

Precision is a measurement of the exactness of the model's classifications. Here, the model predicted 773 reviews as being fake and of those, 92.7$\%$ of them were correct.

\[Recall = \dfrac{TP}{TP + FN} = 0.773\]

Recall, simply put, is the proportion of relevant instances that were correctly classified, which in this case would be fake reviews. This model classified 77.3$\%$ of the actual fake reviews as fake.

The SVM model performs better than a random guess with an accuracy of 78.4$\%$. The misclassification of fake reviews as real is more detrimental than the other way around because in the former scenario, potential customers will base their purchasing decisions on wrong information. Thus, the recall, or the ability of the model to identify fake reviews from the ones that are actually fake, should be maximized. For this model, the recall is at a reasonably high value of 77.3$\%$. Another thing that should be mentioned is the high precision. Specifically, when the model predicts that a review is fake, it is correct 92.7$\%$ of the time using this dataset.

\section{Conclusion}
\label{sec:conclusion}

This paper corroborates the methodologies and findings of prior research papers on this topic. Natural language processing and supervised learning techniques are effective methods of detecting genuine reviews from non-genuine ones. This is important because vendors acting in bad faith may create fake positive reviews to promote their products or generate fake negative reviews to stifle their competition. 

This study did not create any new features from the existing ones, which may be beneficial to improving model performance. Specifically, a number of potentially useful features could be created from the review texts. For instance the number of emojis used, number of capital letters, or number of punctuation marks. Hyper-parameter tuning was also not utilized in this study, which may also benefit generalization performance.

Future work on this study should include engineering of new features, hyper-parameter optimization, and testing on a larger dataset that spans a longer time frame to ensure that it doesn't overfit to a specific interval of time.

\bibliographystyle{chicago}
\bibliography{references.bib}

\end{document}
